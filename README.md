# ğŸŒ Happiness Prediction Project by Country

## ğŸ“œ General Description

This project utilizes happiness data from various countries over several years (2015-2019) to train a regression model that predicts happiness scores. The structure includes a complete workflow, from data exploration and transformation (EDA/ETL) to storing predictions in a database, as well as training and evaluating the model. Additionally, a real-time data streaming system is implemented using Apache Kafka.

## ğŸ¯ Project Objective

The objective is to train a regression model using historical data to predict happiness scores for different countries. The data is analyzed and cleaned through EDA, and the trained model is tested using 70% of the data for training and the remaining 30% for testing. Accuracy is evaluated through performance metrics, and the results are stored in a database for further analysis.

![image](https://github.com/caroldvarela/images/blob/main/workshop3.png)

## ğŸ› ï¸ Technologies Used

- **Programming Language**: Python ğŸ
- **Notebooks**: Jupyter Notebook ğŸ““
- **Machine Learning**: Scikit-learn ğŸ“Š
- **Data Streaming**: Apache Kafka ğŸ“¡
- **Database**: Your choice (e.g., PostgreSQL, MySQL, SQLite) ğŸ’¾
- **Docker**: for deployment and service configuration ğŸ³

## ğŸ“‚ Project Structure

The structure of the project is as follows:

```plaintext
.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ 2015.csv
â”‚   â”œâ”€â”€ 2016.csv
â”‚   â”œâ”€â”€ 2017.csv
â”‚   â”œâ”€â”€ 2018.csv
â”‚   â”œâ”€â”€ 2019.csv
â”‚   â””â”€â”€ data_clean.csv
â”œâ”€â”€ database
â”‚   â”œâ”€â”€ db_utils.py
â”‚   â””â”€â”€ model.py
â”œâ”€â”€ models
â”‚   â””â”€â”€ best_model_Random_forest.pkl
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ 001_EDA_and_model_training.ipynb
â”‚   â””â”€â”€ 002_model_metrics.ipynb
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ data_processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ feature_selection.py
â”‚   â”œâ”€â”€ predictions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ save_predictions.py 
â”‚   â””â”€â”€ streaming
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ kafka_utils.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ main.py
â””â”€â”€ requirements.txt
```

## ğŸ› ï¸ Component Description

- **`data/`** ğŸ“Š: Contains CSV files with information about happiness scores of various countries from 2015 to 2019. Also includes a `data_clean.csv` file that results from data cleaning and transformation.

- **`database/`** ğŸ—„ï¸: Includes `db_utils.py` for setting up and managing the connection to the database, and `model.py` for storing predictions along with their respective features in the database.

- **`models/`** ğŸ¤–: Folder where the best trained model (`best_model_Random_forest.pkl`) is stored, used for making predictions.

- **`notebooks/`** ğŸ““: Folder containing Jupyter notebooks. `001_EDA_and_model_training.ipynb` performs Exploratory Data Analysis (EDA) and model training. `002_model_metrics.ipynb` calculates and analyzes the performance metrics of the model using the test set.

- **`src/`** ğŸ”§: Source code of the project organized into subfolders:
  - **`data_processing/`** ğŸ› ï¸: Code for data manipulation. Contains `feature_selection.py` for selecting the features.
  - **`predictions/`** ğŸ“ˆ: Code for handling predictions generated by the model. `save_predictions.py` is responsible for generating and saving predictions to the database.
  - **`streaming/`** ğŸ“¡: Code for setting up and managing data streaming using Kafka, including `kafka_utils.py` for connection and streaming utilities.

- **`main.py`** ğŸ¬: Main script to execute kafka producer and consumer: data loading, preprocessing, predictions, and storage in the database.

- **`docker-compose.yml`** ğŸ“¦: Configuration file for deploying Kafka and Zookeeper using Docker.

- **`requirements.txt`** ğŸ“œ: File listing the necessary dependencies for the project, which can be installed using pip.


## âš™ï¸ Setup and Execution

First, you need to have a few things installed. Hereâ€™s a brief tutorial on how to do it.

## Installing Docker on Ubuntu <img src="https://upload.wikimedia.org/wikipedia/commons/7/79/Docker_%28container_engine%29_logo.png" alt="Docker Logo" width="70" style="vertical-align: middle;"/>


1. **Update your system**  
   Update the package list and install available updates:
   ```bash
   sudo apt update && sudo apt upgrade -y

2. **Install certificates and data transfer tool**  
   Install the necessary certificates and curl for data transfer:
   ```bash
   sudo apt-get install ca-certificates curl

3. **Create a secure directory for APT repository keys**  
   Create a directory to store the repository keys:
   ```sql
   sudo install -m 0755 -d /etc/apt/keyrings
   
4. **Download and save the Docker GPG key to the system**  
   Download the Docker GPG key and save it in the created directory:
   
   ```bash
   sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
   
5. **Grant read permissions to all users for the Docker GPG key**  
   Allow all users to read the GPG key:
   ```bash
   sudo chmod a+r /etc/apt/keyrings/docker.asc

6. **Add the Docker repository and update the package list**  
   Add the Docker repository to the APT sources and update the package list:
   ```bash
   echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
   sudo apt-get update
   
7. **Install Docker Engine, CLI, Containerd, Buildx, and Compose plugins**  
   Install Docker and its necessary components:
   ```bash
   sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

## Installing PostgreSQL <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Postgresql_elephant.svg/200px-Postgresql_elephant.svg.png" alt="PostgreSQL Logo" width="30" style="vertical-align: middle;"/>

1. **Set Up PostgreSQL**  
   Install and configure PostgreSQL:
   ```bash
   sudo apt update
   sudo apt-get -y install postgresql postgresql-contrib
   sudo service postgresql start
   sudo apt-get install libpq-dev python3-dev

2. **Log in to PostgreSQL**  
   Run the following commands to log in:
   ```bash
   sudo -i -u postgres
   psql

3. **Create a New Database and User**  
   Run the following SQL commands to create a new user and database:
   ```sql
   CREATE USER <your_user> WITH PASSWORD '<your_password>';
   ALTER USER <your_user> WITH SUPERUSER;
   CREATE DATABASE workshop3 OWNER <your_user>;
   

6. **Set Up pgAdmin 4 (Optional)**  
   To install pgAdmin 4, run the following commands:
   ```bash
   sudo apt install curl
   sudo curl https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo apt-key add
   sudo sh -c 'echo "deb https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/$(lsb_release -cs) pgadmin4 main" > /etc/apt/sources.list.d/pgadmin4.list && apt update'
   sudo apt install pgadmin4

## Configuration of the .env file ğŸ“‚âœ¨

- **Create a .env File**  
   Create a `.env` file with the following configuration:
   ```plaintext
   PGDIALECT=postgresql
   PGUSER=<your_user>
   PGPASSWD=<your_password>
   PGHOST=localhost
   PGPORT=5432
   PGDB=workshop3
   WORK_DIR=<your_working_directory>

## Time to setup the project ğŸš€

1. **Clone the Repository**
   ```bash
   git clone https://github.com/caroldvarela/workshop-03.git
   cd workshop-03
   
2. **Create a Virtual Environment**  
   Create a Python virtual environment to manage dependencies:
   ```bash
   python3 -m venv venv
   source venv/bin/activate

3. **Install Dependencies**  
   To install the project's dependencies:
   ```bash
   pip install -r requirements.txt

4. **Configure Kafka**  
   Set up the Kafka service using Docker:
   ```bash
   docker compose up -d

5. **Access Kafka Container**  
   Open a terminal inside the Kafka container:
   ```bash
   docker exec -it kafka bash

6. **Create Kafka Topic**  
   Set up a new Kafka topic for the workshop:
   ```bash
   kafka-topics --bootstrap-server kafka:9092 --create --topic kafka_workshop3
   
7. **Run the Complete Pipeline**  
   Exit the Kafka console, and in the project folder execute the main pipeline:
   ```bash
   python main.py


